# AIAIA Kubeflow deployment to GCP

**Note**: `aiaia_detector_tf1` is built out of TensorFlow version 1.15 and TF Object detection API version 1.

## Getting Started

The first step is to ask a project administrator to add you to the `bp-padang`
project on Google Cloud Platform. Otherwise you will be unable to gain access
to the [GCP Console][1], deploy a cluster via the command line, nor complete the
setup steps described here.

In the meantime:

- [Install the Google Cloud SDK][2]. On macOS, if you have Homebrew installed,
  the easiest way to install the SDK is to simply run
  `brew cask install google-cloud-sdk`

Once the Google Cloud SDK is installed, run the following command to initialize
your `gcloud` environment, and when prompted for the project, enter `bp-padang`:

```bash
gcloud init
```

Next, install the additional required components:

```bash
gcloud components install kpt anthoscli beta
gcloud components update
```

Once an administrator has added you to the Google Cloud Project Connect project, run
the following command to login:

```bash
gcloud auth application-default login
```

Further, in order to be able to push Docker images to the Google Container Registry
(GCR), you must authenticate Docker to do so with the following command (See
<https://cloud.google.com/container-registry/docs/advanced-authentication#gcloud-helper>):

```bash
gcloud auth configure-docker gcr.io
```

You should see output similar to the following, and you should respond to the prompt in
the affirmative (where `<HOME>` is the path to your user home directory):

```plain
Adding credentials for: gcr.io
After update, the following will be written to your Docker config file
 located at [<HOME>/.docker/config.json]:
 {
  "credHelpers": {
    "gcr.io": "gcloud"
  }
}

Do you want to continue (Y/n)? y

Docker configuration file updated.
```

Next, create a `.env` file at the folder the `/aiai_detector ` defining the following
environment variables (don't worry, `.env` is listed in the `.gitignore` file),
where the values for `CLIENT_ID` and `CLIENT_SECRET` are obtained from the file
`~/.config/gcloud/application_default_credentials.json`, which was generated by
the previous command:

If the above credential does not work for display the dashboard create one [here](https://console.cloud.google.com/apis/credentials/oauthclient/),and get the [credentials](https://user-images.githubusercontent.com/1152236/98403067-5ae1c380-2036-11eb-8783-184aaf2f619c.png)

```properties
CLIENT_ID=[See ~/.config/gcloud/application_default_credentials.json]
CLIENT_SECRET=[See ~/.config/gcloud/application_default_credentials.json]
PROJECT=bp-padang
ZONE=us-east1-c
```

Install the following CLI tools:

- `yq`: On macOS, use `brew install yq`. Otherwise, see
  <https://github.com/mikefarah/yq>.
- `stern`: On macOS, use `brew install stern`. For Linux,
  download a [binary release][3] and place it on your `PATH`.

## Run with Kubeflow

### Building the training image

To build the training image, run the following command, replacing `VERSION` with an
appropriate value (e.g., `v2`):

```bash
export VERSION=v1
export PROJECT=bp-padang
docker build . -f Dockerfile-gpu -t gcr.io/${PROJECT}/aiaia:${VERSION}-tf1.15-gpu
docker build . -f Dockerfile-cpu -t gcr.io/${PROJECT}/aiaia:${VERSION}-tf1.15-cpu

```

* Uses a google deep learning base with tensorflow 1.15 and the correct CUDA
  version (10.0) pre-installed -- note that this container does not support tensorflow
  addons package.
* Corresponds to Dockerfile in repo.

<details>
If it's the first time you open the PROJECT before you push the image to GCR, it will ask you to 'Enable' the GCR API on the [Cloud Console](https://console.cloud.google.com/apis/api/containerregistry.googleapis.com/overview?project=bp-padang).

And enable other [API resources](https://www.kubeflow.org/docs/gke/deploy/project-setup/), e.g., running:

```bash

gcloud services enable \
  compute.googleapis.com \
  container.googleapis.com \
  iam.googleapis.com \
  servicemanagement.googleapis.com \
  cloudresourcemanager.googleapis.com \
  ml.googleapis.com
```
</details>
Now, you can push the image to GCR using the following command:

```bash
docker push gcr.io/${PROJECT}/aiaia:${VERSION}-tf1.15-gpu
```


Run docker locally:

```bash
export VERSION=v1
export PROJECT=bp-padang
docker run -u 0 --rm -v ${PWD}://mnt/data -it gcr.io/${PROJECT}/aiaia:${VERSION}-tf1.15-cpu bash

# Once in the container
gcloud init
gcloud auth application-default login

## Run a test
# under aiaia_detector_tf1

python aiai_detector/model_main.py \
    --model_dir=aiaia_od/model_outputs/ssd_mobilenet_v1_wildlife \
    --pipeline_config_path=model_configs/configs/ssd_mobilenet_v1.config \
    --num_train_steps=10 \
    --sample_1_of_n_eval_examples=1 \
    --input_type=image_tensor \
    --output_directory=aiaia_od/export_outputs/ssd_mobilenet_v1_wildlife_export/ \

```
Above scripts will exported model that ready to be containerized as TFServing image.
To build a TFServing image, you will need:
- Download the model files from GCS;
- Containerized the `saved_model` into CPU or GPU version.

Using this [`build.sh`](tf_serving/build.sh) script will do above two steps to build TFServing image for your exported and saved model for the coming ML inference either locally or with Chip n Scale.


In order to deploy the docker images and push into GCR, we can execute.

```
./build_image_gcp.sh
```


### Setup Kubeflow on GCP cluster

Assuming you have correctly specified values in your `.env` file as described
above, as well as installed the prerequisite tools (also described above), you
should be able to deploy a new cluster with the following command. (**NOTE:**
this will automatically detect and install the correct versions of `kubectl`
and `kfctl` for you, so there should be no need for you to do so manually.)

```bash
# TYPE is either 'standard' or 'highmem'
# Run ./deploy without args for details
# the following deployment will start a NVIDIA_K80_GPUS
./deploy --node-pool standard
```

**NOTE:** This will take about 15 minutes to complete!

### Verify Resources

Once deployment is complete, run the following command to see all resources for
your cluster:

```
kubectl -n kubeflow get all
```


### Clean up

The following information will be printed out when the Kubeflow is successfully deployed.
You will need to add `--no-dry-run` to the following bash script to completely delete the cluster and resource once the ML training is finished.

```bash
# Delete cluster/resources once finished
./clean ${KF_DIR}
```

### Deploy TF jobs

If you have already established the optimal combinations of hyper-parameters you can create a tf-jobs yaml file. See yaml file in tf_jobs for example.

```bash
kubectl create -f <path to tf job yaml file>
```

Check if the experiment is deployed properly, by running the following for logging:

```bash
stern -n kubeflow --since 10m --container tensorflow ".*"
```

### Delete TF experiment
To terminate the model experiment, run:

```bash
kubectl delete -f <path to tf job yaml file>
```

### Watching model training with Tensorboard

After an object detection model is trained and model checkpoints saved in GCS. You can go through these few steps to visualize tensorboard:

#### Visualize tensorboard locally

```bash
gcloud init # to sign into your project with your email
gcloud auth application-default login

tensorboard --logdir='gs://aiaia_od/model_outputs_tf1/rcnn_resnet101_serengeti_wildlife_v3/'
```

#### Visualize tensorboard with Tensorboard Dev

```bash
gcloud init # to sign into your project with your email
gcloud auth application-default login

pip3 install -U tensorboard

## This will upload model files to tensorboard dev
tensorboard dev upload --logdir gs://aiaia_od/model_outputs_tf1/rcnn_resnet101_serengeti_wildlife_v3/

```
