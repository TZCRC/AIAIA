# AIAIA Classifier

## Getting Started

The first step is to ask a project administrator to add you to the `AIAIA`
project on Google Cloud Platform. Otherwise you will be unable to gain access
to the [GCP Console](https://console.cloud.google.com/kubernetes/list?project=bp-padang), deploy a cluster via the command line, nor complete the
setup steps described here.

In the meantime:

- [Install the Google Cloud SDK][2]. On macOS, if you have Homebrew installed,
  the easiest way to install the SDK is to simply run
  `brew cask install google-cloud-sdk`

Once the Google Cloud SDK is installed, run the following command to initialize
your `gcloud` environment, and when prompted for the project, enter `AIAIA`:

```bash
gcloud init
```

Next, install the additional required components:

```bash
gcloud components install kpt anthoscli beta
gcloud components update
```

Once an administrator has added you to the Google Cloud Project Connect project, run
the following command to login:

```bash
gcloud auth application-default login
```

Further, in order to be able to push Docker images to the Google Container Registry
(GCR), you must authenticate Docker to do so with the following command (See
<https://cloud.google.com/container-registry/docs/advanced-authentication#gcloud-helper>):

```bash
gcloud auth configure-docker gcr.io
```

You should see output similar to the following, and you should respond to the prompt in
the affirmative (where `<HOME>` is the path to your user home directory):

```plain
Adding credentials for: gcr.io
After update, the following will be written to your Docker config file
 located at [<HOME>/.docker/config.json]:
 {
  "credHelpers": {
    "gcr.io": "gcloud"
  }
}

Do you want to continue (Y/n)? y

Docker configuration file updated.
```

Next, create a `.env` file at the root of this repository defining the following
environment variables (don't worry, `.env` is listed in the `.gitignore` file),
where the values for `CLIENT_ID` and `CLIENT_SECRET` are obtained from the file
`~/.config/gcloud/application_default_credentials.json`, which was generated by
the previous command:

```properties
CLIENT_ID=[See ~/.config/gcloud/application_default_credentials.json]
CLIENT_SECRET=[See ~/.config/gcloud/application_default_credentials.json]
PROJECT=bp-padang
ZONE=us-east1-c
```

Install the following CLI tools:

- `yq`: On macOS, use `brew install yq`. Currently we use version 4.2.1. Otherwise, see
  <https://github.com/mikefarah/yq>.
- `stern`: On macOS, use `brew install stern`. For Linux,
  download a [binary release][3] and place it on your `PATH`.

## Run with Kubeflow

### Building the training image

To build the training image, run the following command, replacing `VERSION` with an
appropriate value (e.g., `v2`):

```bash
export VERSION=v1
export PROJECT=bp-padang
docker build . -t gcr.io/${PROJECT}/aiaia-classifier:${VERSION}-xception-binary
```

* Uses a google deep learning base with tensorflow 2 and the correct CUDA
  version (10.0) pre-installed -- note that this container does not support tensorflow
  addons package.
* Corresponds to Dockerfile in repo.

If it's the first time you open the PROJECT before you push the image to GCR, it will ask you to 'Enable' the GCR API on the Cloud Console, e.g. 'https://console.cloud.google.com/apis/api/containerregistry.googleapis.com/overview?project=project-connect-289520'
And enable other [API resources](https://www.kubeflow.org/docs/gke/deploy/project-setup/), e.g., running:

```bash

gcloud services enable \                         
  compute.googleapis.com \
  container.googleapis.com \
  iam.googleapis.com \
  servicemanagement.googleapis.com \
  cloudresourcemanager.googleapis.com \
  ml.googleapis.com
```

Now, you can push the image to GCR using the following command:

```bash
docker push gcr.io/${PROJECT}/aiaia-classifier:${VERSION}-xception-binary
```

Make sure that the docker info in the `katib` file matches the most up-to date
version of the dockerfile.

### Setup Kubeflow on GCP cluster

Assuming you have correctly specified values in your `.env` file as described
above, as well as installed the prerequisite tools (also described above), you
should be able to deploy a new cluster with the following command. (**NOTE:**
this will automatically detect and install the correct versions of `kubectl`
and `kfctl` for you, so there should be no need for you to do so manually.)

```bash
# TYPE is either 'standard' or 'highmem'
# Run ./deploy without args for details
./deploy --node-pool standard
```

**NOTE:** This will take about 15 minutes to complete!

### Verify Resources

Once deployment is complete, run the following command to see all resources for
your cluster:

```
kubectl -n kubeflow get all
```

### Single Experiment Run using TF-job

If you have already established the optimal combinations of hyper-parameters you
can create a tf-jobs yaml file. See yaml file in tf_jobs for example.

### Start Experiment
Before you start the experiment, the tf_job yaml file will need to be updated, particularly the `tf_train_data_dir` and `tf_val_data_dir`.

```
kubectl create -f <path to tf job yaml file>
```

Check if the experiment is deployed properly, by running the following for logging:

```bash
stern -n kubeflow --since 10m --container tensorflow ".*"

```
### Deploying hyperparameter optimization experiments

```
kubectl create -f <path to yaml file>

# see trials status
kubectl describe experiment <experiment_name> -n kubeflow

# see hyper-parameter combinations katib has generated
kubectl -n kubeflow describe suggestions

# useful to start looking at after trials have completed
kubectl -n kubeflow port-forward svc/katib-ui 8080:80
# then go to http://localhost:8080/katib/#/katib/hp_monitor for visualizations

# Delete running experiments like:
kubectl delete -f <path to yaml file>
```

### Clean up

The following information will be printed out when the Kubeflow is successfully deployed.
You will need to add `--no-dry-run` to the following bash script to completely delete the cluster and resource once the ML training is finished.

```bash
# Delete cluster/resources once finished
./clean ${KF_DIR}
```
## Model evaluation

### Running testing locally
Download TFRecords down from GCP either change the flags in aiaia_classifier/eval.py code.
Download the model checkpoint files from the best performing training step locally.
You can run the following command under `aiaia_classifier` directory:

```bash
python3 aiaia_classifier/eval.py \
       --tf_test_data_dir=dir_path_to_test_tfrecords/ \
       --country={country_name} \
       --tf_test_ckpt_path=dir_path_to_model_checkpoints/model.ckpt-6000 \
       --tf_test_results_dir=local_dir4_model_eval
```
*Note*: currently, model parameters under `eval.py` is hard coded for this aiaia classifier model.

### Reading files from GCS for model testing

You can also point the model check point and test dataset that hosted on GCS. To do so, you will need to install packages:
- `pip3 install google-cloud-bigquery tenacity`, and
- log in to your GCP with `gcloud` and application authentication shows as follows:

```bash
# log in to the GCP so we can access to the files on GCS

gcloud init

gcloud auth application-default login

python3 aiaia_classifier/eval.py \
        --tf_test_data_dir='gs://dir_path_to_test_tfrecords/' \
        --countries={country_name} \
        --tf_test_ckpt_path='gs://dir_path_to_model_checkpoints/model_outputs/v1/model.ckpt-6000' \
        --tf_test_results_dir=local_dir4_model_eval

python3 aiaia_classifier/eval.py \
        --tf_test_data_dir='gs://aiaia_od/training_data_aiaia_p400/classification_training_tfrecords/' \
        --countries=aiaia \
        --tf_test_ckpt_path='gs://aiaia_od/classification_model_outputs/abc/model.ckpt-6000' \
        --tf_test_results_dir=local_dir4_model_eval
```

The above command will output three files:
- `preds.csv`;
- `test_stats.csv`; and
- `dist_fpr_tpr_{countries}.png` and `roc_{countries}.png` that shows model true and false positive rate and roc curve.
